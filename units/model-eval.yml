title: Model evaluation
theme: model
needs: model-multivariate
readings: ~
updated: ~
desc: "\n``` r\nlibrary(tidyverse)\nlibrary(modelr)\n```\n\nA crucial part of the
  modeling process is determining the quality of\nyour model. In this reading, we’ll
  explore how to compare different\nmodels to each other and pick the one that best
  approximates your data.\n\n## The problem\n\nIn order to illustrate the process
  of model evaluation, we’re going to\nuse simulated data. To simulate data, you pick
  a function and use that\nfunction to generate a series of data points. We can then
  compare any\nmodels we make to the true function used to generate the data.\n\nNote
  that you almost always won’t actually know the functional\nrepresentation underlying
  your data. If you did, you probably wouldn’t\nneed a model in the first place.\n\nFor
  our simulated data, let’s assume there’s some phenomenon that has a\nfunctional
  representation of the form\n\n    y = f(x)\n\nIn other words, the phenomenon is
  purely a function of the variable `x`,\nand so, if you know `x`, you can figure
  out the behavior of the\nphenomenon. Here’s the function `f()` that we’ll use.\n\n```
  r\n# True function\nf <- function(x) x + 50 * sin((pi / 50) * x)\n```\n\n`f()` is
  the true function, but let’s also assume that any measurements\nof `y` will have
  errors. To simulate errors, we need a function that\nadds random noise to `f()`.
  We’ll call this function `g()`.\n\n``` r\n# Function with measurement error\ng <-
  function(x) f(x) + rnorm(n = length(x), mean = 0, sd = 20)\n```\n\nWe can use `g`
  to generate a random sample of data.\n\n``` r\n# Function that generates a random
  sample of data points, using g(x)\nsim_data <- function(from, to, by) {\n  tibble(\n
  \   x = seq(from = from, to = to, by = by),\n    y = g(x)\n  )\n}\n\n# Generate
  random sample of points to approximate\nset.seed(439)\ndata_1 <- sim_data(0, 100,
  0.5)\n```\n\nHere’s a plot of both the simulated data (`data_1`), and the true\nfunction.
  Our job is now to use the simulated data to create a function\nthat closely approximates
  the true function.\n\n``` r\ntibble(x = seq(0, 100, 0.5), y = f(x)) %>% \n  ggplot(aes(x,
  y)) +\n  geom_line() +\n  geom_point(data = data_1) +\n  labs(title = \"True function
  and simulated data\")\n```\n\n![](model-eval_files/figure-gfm/unnamed-chunk-5-1.png)<!--
  -->\n\n## The `loess()` model\n\nGiven the shape of our data, the `stats::loess()`
  function is a good\nplace to start. `loess()` works by creating a local model at
  each data\npoint. Each of these local models only uses data within a given distance\nfrom
  that point. The `span` parameter controls how many points are\nincluded in each
  of these local models. The smaller the span, the fewer\nthe number of points included
  in each local model.\n\nTo better understand the effect of the `span` parameter,
  let’s plot\n`loess()` models with different values of `span`.\n\nThe function `plot_loess()`
  fits a model using `loess()`, then plots the\nmodel predictions and the underlying
  data.\n\n``` r\n# Plot loess model for given span\nplot_loess <- function(span)
  {\n  data_1 %>% \n    mutate(f = f(x)) %>% \n    add_predictions(loess(y ~ x, data
  = data_1, span = span)) %>% \n    ggplot(aes(x)) +\n    geom_line(aes(y = f)) +\n
  \   geom_point(aes(y = y)) +\n    geom_line(aes(y = pred), color = \"#3366FF\",
  size = 1) +\n    labs(\n      title = str_glue(\"loess, span = {span}\"),\n      y
  = \"y\"\n    )\n}\n```\n\nNow, we can use purrr to apply `plot_loess()` to a vector
  of different\nspan values.\n\n``` r\nc(10, 1, 0.75, 0.1, 0.01) %>% \n  map(plot_loess)
  %>% \n  walk(print)\n```\n\n![](model-eval_files/figure-gfm/unnamed-chunk-7-1.png)<!--
  -->![](model-eval_files/figure-gfm/unnamed-chunk-7-2.png)<!-- -->![](model-eval_files/figure-gfm/unnamed-chunk-7-3.png)<!--
  -->![](model-eval_files/figure-gfm/unnamed-chunk-7-4.png)<!-- -->![](model-eval_files/figure-gfm/unnamed-chunk-7-5.png)<!--
  -->\n\nThe `span` parameter clearly makes a big difference in the model:\n\n  -
  `span = 10` smooths too much and washes out the variability in the\n    model.\n
  \ - `span = 1` is much better, but still has a bit too much smoothing.\n  - `span
  = 0.75` is the default and does a reasonably good job.\n  - `span = 0.1` has too
  little smoothing. The added complexity has\n    started tracking the random variation
  in our data.\n  - `span = 0.01` went way too far and started to fit individual random\n
  \   points.\n\nIt’s clear that the model with `span = 0.75` comes closest to following\nthe
  form of the true model. The models with `span` equal to `0.1` and\n`0.01` have **overfit**
  the data, and so won’t generalize well to new\ndata.\n\n## Test and training errors\n\nLet’s
  return to the question of how to measure the quality of a model.\nAbove, we could
  easily visualize our different models because they each\nhad only one predictor
  variable, `x`. It will be much harder to\nvisualize models with many predictor variables,
  and so we need a more\ngeneral way of determining model quality and comparing models
  to each\nother.\n\nOne way to measure model quality involves comparing the model’s\npredicted
  values to the actual values in data. Ideally, we want to know\nthe difference between
  predicted and actual values on *new* data. This\ndifference is called the **test
  error**.\n\nUnfortunately, you usually won’t have new data. All you’ll have is the\ndata
  used to create the model. The difference between the predicted and\nactual values
  on the data used to train the model is called the\n**training error**.\n\nSince
  the expected value for the error term of `g()` is `0` for each\n`x`, the best possible
  model for new data would be `f()` itself. The\nerror on `f()` itself is called the
  **Bayes error**.\n\nUsually, you won’t know the true function `f()`, and you won’t
  have\naccess to new test data. In our case, we simulated data, so we know\n`f()`
  and can generate new data. The following code creates 50 different\nsamples, each
  the same size. We’ll use these 50 different samples to get\nan accurate estimate
  of our test error.\n\n``` r\nset.seed(886)\n\ndata_50 <- \n  1:50 %>% \n  map_dfr(~
  sim_data(0, 100, 0.5))\n```\n\nNow, we need a way to measure the difference between
  a vector of\npredictions and vector of actual values. Two common metrics include:\n\n
  \ - Root-mean-square error (RMSE): `sqrt(mean((y - pred)^2, na.rm =\n    TRUE))`\n
  \ - Mean absolute error (MAE): `mean(abs(y - pred), na.rm = TRUE)`\n\nBoth measures
  are supported by modelr. We’ll use RMSE below.\n\nNow, we’ll train a series of models
  on our original dataset, `data_1`,\nusing a range of `span`’s. Then, for each model,
  we’ll calculate the\ntraining error on `data_1` and the test error on the new data
  in\n`data_50`.\n\nFirst, we need to create a vector of different `span` values.\n\n```
  r\nspans <- 2^seq(1, -2, -0.2)\n```\n\nNext, we can use purrr to iterate over the
  span values, fit a model for\neach one, and calculate the RMSE.\n\n``` r\nerrors
  <- \n  tibble(span = spans) %>% \n  mutate(\n    train_error =\n      map_dbl(span,
  ~ rmse(loess(y ~ x, data = data_1, span = .), data_1)),\n    test_error =\n      map_dbl(span,
  ~ rmse(loess(y ~ x, data = data_1, span = .), data_50))\n  )\n```\n\nThe result
  is a tibble with a training error and test error for each\n`span`.\n\n``` r\nerrors\n```\n\n
  \   ## # A tibble: 16 x 3\n    ##     span train_error test_error\n    ##    <dbl>
  \      <dbl>      <dbl>\n    ##  1 2            27.7       27.4\n    ##  2 1.74
  \        26.9       26.7\n    ##  3 1.52         25.9       25.7\n    ##  4 1.32
  \        24.6       24.6\n    ##  5 1.15         22.8       23.0\n    ##  6 1            21.3
  \      21.8\n    ##  7 0.871        19.3       20.4\n    ##  8 0.758        18.7
  \      20.2\n    ##  9 0.660        18.5       20.2\n    ## 10 0.574        18.4
  \      20.3\n    ## 11 0.5          18.4       20.3\n    ## 12 0.435        18.3
  \      20.4\n    ## 13 0.379        18.1       20.4\n    ## 14 0.330        17.8
  \      20.4\n    ## 15 0.287        17.4       20.6\n    ## 16 0.25         17.2
  \      20.8\n\nIt would be useful to compare these errors to the Bayes error, which
  you\ncan think of as the “best-case-scenario” error. The following code\ncalculates
  the Bayes error from `data_50`.\n\n``` r\nbayes_error <- \n  data_50 %>% \n  summarize(bayes_error
  = sqrt(mean((y - f(x))^2, na.rm = TRUE))) %>% \n  pull(bayes_error)\n\nbayes_error
  \n```\n\n    ## [1] 19.9741\n\nNow, let’s plot the training, test, and Bayes errors
  by `span`.\n\n``` r\nbayes <- tibble(\n  span = range(errors$span),\n  type = \"bayes_error\",\n
  \ error = bayes_error\n)\n\nerrors %>% \n  gather(key = type, value = error, -span)
  %>% \n  ggplot(aes(1 / span, error, color = type)) + \n  geom_line(data = bayes)
  +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Errors as a function
  of increasing model complexity\",\n    color = NULL\n  )\n```\n\n![](model-eval_files/figure-gfm/unnamed-chunk-13-1.png)<!--
  -->\n\nMapping 1 / `span` to the x-axis makes the plot easier to interpret\nbecause
  smaller values of `span` create more complex models. The\nleftmost points indicate
  the simplest models, and the rightmost indicate\nthe most complex.\n\nThe lowest
  test error is around 20.18, and came from the model with\n`span` ≈ 0.758. Note that
  the default value of `span` in `loess()` is\n0.75, so the default here would have
  done pretty well.\n\nYou can see that as the model increases in complexity, the
  training\nerror (blue line) continually declines, but the test error (green line)\nactually
  starts to increase. The test error increases for values of\n`span` that make the
  model too complex overfit the training data.\n\nWe used `data_50`, our new data,
  to calculate the actual test error. As\nwe’ve said, though, you usually won’t have
  access to new data and so\nwon’t be able to calculate the test error. You’ll need
  a way to\nestimate the test error using the training data. As our plot indicates,\nhowever,
  the training error is a poor estimate of the actual test error.\nIn the next section,
  we’ll discuss a better way, called\n*cross-validation*, to estimate the test error
  from the training data.\n\n## Cross-validation\n\nThere are two key ideas of cross-validation.
  First, in the absence of\nnew data, we can hold out of a random portion of our original
  data (the\ntest set), train the model on the rest of the data (the training set),\nand
  then test the model on test set. Second, we can generate multiple\ntrain-test pairs
  to create a more robust estimate of the true test\nerror.\n\nmodelr provides two
  functions for generating these train-test pairs:\n`crossv_mc()` and `crossv_kfold()`.
  We’ll introduce these functions in\nthe next section.\n\n### Generating train-test
  paires with `crossv_mc()` and `crossv_kfold()`\n\n`crossv_mc()` and `crossv_kfold()`
  both take your original data as input\nand produce a tibble that looks like this:\n\n```
  r\ndata_1 %>% \n  crossv_kfold()\n```\n\n    ## # A tibble: 5 x 3\n    ##   train
  \         test           .id  \n    ##   <list>         <list>         <chr>\n    ##
  1 <S3: resample> <S3: resample> 1    \n    ## 2 <S3: resample> <S3: resample> 2
  \   \n    ## 3 <S3: resample> <S3: resample> 3    \n    ## 4 <S3: resample> <S3:
  resample> 4    \n    ## 5 <S3: resample> <S3: resample> 5\n\nEach row represents
  one train-test pair. Eventually, we’re going to\nbuild a model on each train portion,
  and then test that model on each\ntest portion.\n\n`crossv_mc()` and `crossv_kfold()`
  differ in how they generate the\ntrain-test pairs.\n\n`crossv_mc()` takes two parameters:
  `n` and `test`. `n` specifies how\nmany train-test pairs to generate. `test` specifies
  the portion of the\ndata to hold out each time. Importantly, `crossv_mc()` pulls
  randomly\nfrom the entire dataset to create each train-test split. This means that\nthe
  test sets are not mutually exclusive. A single point can be present\nin multiple
  test sets.\n\n`crossv_kfold()` takes one parameter: `k`, and splits the data into
  `k`\nexclusive partitions, or *folds*. It then uses each of these `k` folds\nas
  a test set, with the remaining `k` - 1 folds as the training set. The\nmain difference
  between `crossv_mc()` and `crossv_kfold()` is that the\n`crossv_kfold()` test sets
  are mutually exclusive.\n\nNow, let’s take a closer look at how these functions
  works. The\nfollowing code uses `crossv_kfold()` to create 10 train-test pairs.\n\n```
  r\nset.seed(122)\n\ndf <- \n  data_1 %>% \n  crossv_kfold(k = 10)\n\ndf\n```\n\n
  \   ## # A tibble: 10 x 3\n    ##    train          test           .id  \n    ##
  \   <list>         <list>         <chr>\n    ##  1 <S3: resample> <S3: resample>
  01   \n    ##  2 <S3: resample> <S3: resample> 02   \n    ##  3 <S3: resample> <S3:
  resample> 03   \n    ##  4 <S3: resample> <S3: resample> 04   \n    ##  5 <S3: resample>
  <S3: resample> 05   \n    ##  6 <S3: resample> <S3: resample> 06   \n    ##  7 <S3:
  resample> <S3: resample> 07   \n    ##  8 <S3: resample> <S3: resample> 08   \n
  \   ##  9 <S3: resample> <S3: resample> 09   \n    ## 10 <S3: resample> <S3: resample>
  10\n\nThe result is a tibble with two list-columns: `train` and `test`. Each\nelement
  of `train` and `test` is a resample object, which you might not\nhave seen before.
  Let’s look at the first row, the first train-test\npair.\n\n``` r\ntrain_01 <- df$train[[1]]\ntrain_01\n```\n\n
  \   ## <resample [180 x 2]> 1, 3, 4, 5, 6, 7, 9, 10, 11, 12, ...\n\n``` r\ntest_01
  <- df$test[[1]]\ntest_01\n```\n\n    ## <resample [21 x 2]> 2, 8, 21, 43, 53, 63,
  78, 81, 85, 101, ...\n\nEach resample object is actually a two-element list.\n\n```
  r\nlength(train_01)\n```\n\n    ## [1] 2\n\nThe first element, `data` is the original
  dataset.\n\n``` r\ntrain_01$data\n```\n\n    ## # A tibble: 201 x 2\n    ##        x
  \     y\n    ##    <dbl>  <dbl>\n    ##  1   0    22.9 \n    ##  2   0.5  36.6 \n
  \   ##  3   1    -2.32\n    ##  4   1.5  21.3 \n    ##  5   2    18.9 \n    ##  6
  \  2.5  13.0 \n    ##  7   3   -18.3 \n    ##  8   3.5  18.5 \n    ##  9   4    29.0
  \n    ## 10   4.5   9.73\n    ## # … with 191 more rows\n\nThe second element, `idx`,
  is a set of indices that indicate the subset\nof the original data to use.\n\n```
  r\ntrain_01$idx\n```\n\n    ##   [1]   1   3   4   5   6   7   9  10  11  12  13
  \ 14  15  16  17  18  19\n    ##  [18]  20  22  23  24  25  26  27  28  29  30  31
  \ 32  33  34  35  36  37\n    ##  [35]  38  39  40  41  42  44  45  46  47  48  49
  \ 50  51  52  54  55  56\n    ##  [52]  57  58  59  60  61  62  64  65  66  67  68
  \ 69  70  71  72  73  74\n    ##  [69]  75  76  77  79  80  82  83  84  86  87  88
  \ 89  90  91  92  93  94\n    ##  [86]  95  96  97  98  99 100 102 103 104 105 106
  107 108 109 111 112 114\n    ## [103] 115 116 117 118 119 121 122 123 124 125 126
  127 128 130 131 132 133\n    ## [120] 134 135 136 137 138 139 140 142 143 144 145
  147 148 150 151 153 154\n    ## [137] 155 156 157 158 159 162 163 164 165 166 167
  168 169 170 171 172 173\n    ## [154] 174 175 176 177 178 179 180 182 183 184 185
  186 187 188 189 190 191\n    ## [171] 192 193 194 195 196 197 198 199 200 201\n\nWe
  can use a set operation to verify that the train and test sets are\nmutually exclusive.
  In other words, there is no point in a given train\nset that is also in its corresponding
  test set, and vice versa.\n\n``` r\nis_empty(intersect(train_01$idx, test_01$idx))\n```\n\n
  \   ## [1] TRUE\n\nThe union of each train and test set is just the complete original\ndataset.
  We can verify this by checking that the set of all train and\ntest indices contains
  every possible index in the original dataset.\n\n``` r\nsetequal(union(train_01$idx,
  test_01$idx),  1:201)\n```\n\n    ## [1] TRUE\n\nresample objects are not tibbles.
  Some model functions, like `lm()`, can\ntake resample objects as input, but `loess()`
  cannot. In order to use\n`loess()`, we’ll need to turn the resample objects into
  tibbles.\n\n``` r\nas_tibble(test_01)\n```\n\n    ## # A tibble: 21 x 2\n    ##
  \       x     y\n    ##    <dbl> <dbl>\n    ##  1   0.5  36.6\n    ##  2   3.5  18.5\n
  \   ##  3  10    26.8\n    ##  4  21    79.3\n    ##  5  26    68.5\n    ##  6  31
  \  102. \n    ##  7  38.5  76.4\n    ##  8  40    73.6\n    ##  9  42    37.1\n
  \   ## 10  50    40.5\n    ## # … with 11 more rows\n\nResample objects waste a
  lot of space, since each one contains the full\ndataset. A new package [rsample](https://topepo.github.io/rsample/)
  is\nbeing developed to support tidy modeling in a more space-efficient way.\n\n###
  Process\n\n`crossv_kfold()` and `crossv_mc()` just generate train-test pairs. In\norder
  to carry out cross-validation, you still need to fit your model on\neach train set
  and then evaluate it on each corresponding test set. In\nthis section, we’ll walk
  you through the process.\n\n**1** Create a function to calculate your test error\n\nFirst,
  you’ll need a function that calculates the test error using some\nmetric. We’ll
  use RMSE. The following function returns the RMSE for a\ngiven span, train set,
  and test set.\n\n``` r\nrmse_error <- function(span, train, test) {\n  rmse(loess(y
  ~ x, data = as_tibble(train), span = span), as_tibble(test))\n}\n```\n\n**2** Create
  a function to calculate CV error\n\nNext, you need a function that iterates over
  all your train-test pairs,\ncalculates the test error for each pair, and then calculates
  the mean\nand standard deviation of all the errors.\n\n``` r\nspan_error <- function(span,
  data_cv) {\n  errors <- \n    data_cv %>% \n    select(-.id) %>% \n    add_column(span
  = span) %>% \n    pmap_dbl(rmse_error)\n  \n  tibble(\n    span = span,\n    error_mean
  = mean(errors, rm = TRUE),\n    error_sd = sd(errors, na.rm = TRUE)\n  )\n}\n```\n\n**3**
  Create your train-test pairs\n\nUse either `crossv_mc()` or `crossv_kfold()` to
  generate your train-test\npairs. We’ll use `crossv_mc()` to generate 100 train-test
  pairs with\ntest sets consisting of approximately 20% of the data.\n\n``` r\nset.seed(430)\n\ndata_mc
  <- crossv_mc(data_1, n = 100, test = 0.2)\n```\n\n**4** Calculate the CV error for
  different values of your tuning\nparameter\n\nWe want to compare different values
  of `span`, so we also need to\niterate over a vector of different `span` values.
  We’ll calculate the CV\nerror for all `span`s using these train-test pairs.\n\n```
  r\nerrors_mc <- \n  spans %>% \n  map_dfr(~ span_error(span = ., data_cv = data_mc))\n\nerrors_mc
  %>% \n  knitr::kable()\n```\n\n|      span | error\\_mean | error\\_sd |\n| --------:
  | ----------: | --------: |\n| 2.0000000 |    28.13057 |  2.578456 |\n| 1.7411011
  |    27.32540 |  2.572414 |\n| 1.5157166 |    26.29811 |  2.566547 |\n| 1.3195079
  |    25.00708 |  2.558990 |\n| 1.1486984 |    23.06875 |  2.568471 |\n| 1.0000000
  |    21.62002 |  2.532316 |\n| 0.8705506 |    19.58527 |  2.417289 |\n| 0.7578583
  |    18.97282 |  2.323062 |\n| 0.6597540 |    18.84846 |  2.269925 |\n| 0.5743492
  |    18.86447 |  2.256472 |\n| 0.5000000 |    18.90930 |  2.260496 |\n| 0.4352753
  |    18.93128 |  2.254325 |\n| 0.3789291 |    18.88556 |  2.220315 |\n| 0.3298770
  |    18.76699 |  2.171678 |\n| 0.2871746 |    18.60700 |  2.135667 |\n| 0.2500000
  |    18.52702 |  2.109205 |\n\n**5** Choose the best model\n\nNow, we need to compare
  the different models and choose the best value\nof `span`.\n\nBecause we simulated
  our data, we can compare the CV estimates of the\ntest error with the actual test
  error.\n\n``` r\nerrors_mc %>% \n  left_join(errors, by = \"span\") %>% \n  ggplot(aes(1
  / span)) +\n  geom_line(aes(y = test_error, color = \"test_error\")) +\n  geom_point(aes(y
  = test_error, color = \"test_error\")) +\n  geom_line(aes(y = error_mean, color
  = \"mc_error\")) +\n  geom_pointrange(\n    aes(\n      y = error_mean, \n      ymin
  = error_mean - error_sd,\n      ymax = error_mean + error_sd,\n      color = \"mc_error\"\n
  \   )\n  ) +\n  labs(\n    title = \"Cross-validation and test errors\",\n    y
  = \"error\",\n    color = NULL\n  )\n```\n\n![](model-eval_files/figure-gfm/unnamed-chunk-27-1.png)<!--
  -->\n\nFrom this plot, we can see that CV error estimates from `crossv_mc()`\nunderestimate
  the true test error in the range with 1 / `span` \\>= 1.\nThe line ranges reflect
  one standard error on either side of the mean\nerror and show considerable uncertainty
  in the error estimates. Except\nfor the largest 1 / `span` = 4, the test error is
  within one standard\nerror of the mean.\n\nUsually, you won’t have access to the
  true test error, so we need a way\nto choose the best value of `span` using just
  the CV errors. Here’s the\nrule-of-thumb for choosing a tuning parameter when you
  only know the CV\nerrors:\n\n  - Start with the parameter that has the lowest mean
  CV error. In this\n    case, that would be `span` = 0.25 (on the plot, `1/span`
  = 4).\n  - Now, imagine sliding the one-standard-error range for `span` = 0.25\n
  \   all the way to the left of the plot. Then, start sliding the range\n    to the
  right until the first CV mean error is within its bounds. In\n    our case, the
  top of the one-standard-error range for `span` = 0.25\n    is approximately 20.6.
  The CV mean error for `span` = 1 is larger\n    than this, but the next most complex
  model with `span` ≈ 0.871 is\n    smaller, so we would choose this `span`.\n\nWe
  saw above that if we knew the test error, we would choose `span` ≈\n0.758 as the
  optimal parameter. But we usually don’t know the test\nerror, and in this case the
  test errors for 0.871 and 0.758 were very\nclose, with both quite close to the Bayes
  error of 20.\n\n``` r\nerrors %>% \n  filter(span < 0.9, span > 0.7) %>% \n  select(span,
  test_error) %>% \n  knitr::kable()\n```\n\n|      span | test\\_error |\n| --------:
  | ----------: |\n| 0.8705506 |     20.4014 |\n| 0.7578583 |     20.1787 |\n"
