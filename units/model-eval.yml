title: Model evaluation
theme: model
needs: model-multivariate
readings: ~
updated: ~
desc: "\n``` r\nlibrary(tidyverse)\nlibrary(modelr)\n```\n\nA crucial part of the
  modeling process is determining the quality of\nyour model. Often, you’ll want to
  compare different models to each other\nto figure out which one fits your data the
  best. In this reading, we’ll\nexplore how to compare different models to each other
  and pick the\nhighest quality one.\n\n## The problem\n\nIn order to illustrate the
  process of model evaluation, we’re going to\nuse simulated data. To simulate data,
  you pick a function and use that\nfunction to generate a series of data points.
  The advantage of using\nsimulated data is therefore that we actually know the functional\nrepresentation
  of the data, since we were the ones to create the data.\nWe can then compare any
  models we make to the true function.\n\nNote that you almost always won’t actually
  know the functional\nrepresentation of your data. If you did, you probably wouldn’t
  need a\nmodel in the first place.\n\nFor our simulated data, let’s assume there’s
  some phenomenon that has a\nfunctional representation of the form\n\n\\[y = f(x)\\]\n\nIn
  other words, the phenomenon is purely a function of the variable `x`,\nand so, if
  you know `x`, you can figure out the behavior of the\nphenomenon. Here’s the \\(f\\)
  that we’ll use in this reading.\n\n``` r\n# True function\nf <- function(x) x +
  50 * sin((pi / 50) * x)\n```\n\n\\(f\\) is the true function, but let’s also assume
  that any measurements\nof \\(y\\) will have errors. To simulate errors, we need
  a function that\nadds random noise to \\(f\\). We’ll call this function \\(g\\).\n\n```
  r\nset.seed(439)\n\n# Function with measurement error\ng <- function(x) f(x) + rnorm(n
  = length(x), mean = 0, sd = 20)\n```\n\nWe can use \\(g\\) to generate a random
  sample of data.\n\n``` r\n# Function that creates a random sample of data points,
  using g(x)\nsim_data <- function(from, to, by) {\n  tibble(\n    x = seq(from =
  from, to = to, by = by),\n    y = g(x)\n  )\n}\n\ndata_1 <- sim_data(0, 100, 0.5)\n```\n\nHere’s
  a plot of both the simulated data (`data_1`), and the true\nfunction. Our job is
  now to use the simulated data to create a function\nthat hopefully comes close to
  matching the true function.\n\n``` r\ntibble(x = seq(0, 100, 0.5), y = f(x)) %>%
  \n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_point(data = data_1) +\n  labs(title
  = \"True function and simulated data\")\n```\n\n![](model-eval_files/figure-gfm/unnamed-chunk-5-1.png)<!--
  -->\n\n## The `loess()` model\n\nGiven the shape of our data, the `stats::loess()`
  function is a good\nplace to start. `loess()` works by by creating a local model
  at each\ndata point. Each of these local models only uses data within a given\ndistance
  from that point. The `span` parameter controls how many points\nare included in
  each of these local models. The smaller the span, the\nfewer the number of points
  included in each local model.\n\nTo better understand the effect of the `span` parameter,
  let’s plot\n`loess()` models with different values of `span`.\n\nThe function `plot_loess()`
  fits a model using `loess()`, then plots the\nmodel predictions and the underlying
  data.\n\n``` r\n# Plot loess model for given span\nplot_loess <- function(span)
  {\n  data_1 %>% \n    mutate(f = f(x)) %>% \n    add_predictions(loess(y ~ x, data
  = data_1, span = span)) %>% \n    ggplot(aes(x)) +\n    geom_line(aes(y = f)) +\n
  \   geom_point(aes(y = y)) +\n    geom_line(aes(y = pred), color = \"#3366FF\",
  size = 1) +\n    labs(\n      title = str_c(\"loess, span = \", span),\n      y
  = \"y\"\n    )\n}\n```\n\nNow, we can use purrr to apply `plot_loess()` to a vector
  of different\nspan values.\n\n``` r\nc(10, 1, 0.75, 0.1, 0.01) %>% \n  map(plot_loess)
  %>% \n  walk(print)\n```\n\n![](model-eval_files/figure-gfm/unnamed-chunk-7-1.png)<!--
  -->![](model-eval_files/figure-gfm/unnamed-chunk-7-2.png)<!-- -->![](model-eval_files/figure-gfm/unnamed-chunk-7-3.png)<!--
  -->![](model-eval_files/figure-gfm/unnamed-chunk-7-4.png)<!-- -->![](model-eval_files/figure-gfm/unnamed-chunk-7-5.png)<!--
  -->\n\nThe `span` parameter clearly makes a big difference in the model:\n\n  -
  `span = 10` smooths too much and washes out the variability in the\n    model.\n
  \ - `span = 1` is much better, but still has a bit too much smoothing.\n  - `span
  = 0.75` is the default and does a reasonably good job.\n  - `span = 0.1` has too
  little smoothing. The added complexity has\n    started tracking the random variation
  in our data.\n  - `span = 0.01` went way too far and started to fit individual random\n
  \   points.\n\nIt’s clear that the model with `span = 0.75` comes closest to following\nthe
  form of the true model. The models with `span` equal to `0.1` and\n`0.01` have **overfit**
  the data, and so won’t generalize well to new\ndata.\n\n## Test and training errors\n\nLet’s
  return to the question of how to measure the quality of a model.\nAbove, we could
  easily visualize our different models because they each\nhad only one predictor
  variable, `x`. It will be much harder to\nvisualize models with many predictor variables,
  and so we need a more\ngeneral way of determining model quality and comparing models
  to each\nother.\n\nOne way to measure model quality involves comparing the model’s\npredicted
  values to the actual values in data. Ideally, we want to know\nthe difference between
  predicted and actual values on *new* data. This\ndifference is called the **test
  error**.\n\nUnfortunately, you usually won’t have new data. All you’ll have is the\ndata
  used to create the model. The difference between the predicted and\nactual values
  on the data used to train the model is called the\n**training error**.\n\nSince
  the expected value for the error term of `g()` is `0` for each\n`x`, the best possible
  model for new data would be `f()` itself. The\nerror on `f()` itself is called the
  **Bayes error**.\n\nUsually, you won’t know the true function `f()`, and you won’t
  have\naccess to new test data. In our case, we simulated data, so we know\n`f()`
  and can generate new data. The following code creates 50 different\nsamples, each
  the same size. We’ll use these 50 different samples to get\nan accurate estimate
  of our test error.\n\n``` r\nset.seed(886)\n\ndata_50 <- \n  1:50 %>% \n  map_dfr(~
  sim_data(0, 100, 0.5))\n```\n\nNow, we need a way to measure the difference between
  a vector of\npredictions and vector of actual values. Two common metrics include:\n\n
  \ - Root-mean-square error (RMSE): `sqrt(mean((y - pred)^2, na.rm =\n    TRUE))`\n
  \ - Mean absolute error (MAE): `mean(abs(y - pred), na.rm = TRUE)`\n\nBoth measures
  are supported by modelr. We’ll use RMSE below.\n\nNow, we’ll train a series of models
  on our original dataset, `data_1`,\nusing a range of `span`’s. Then, for each model,
  we’ll calculate the\ntraining error on `data_1` and the test error on the new data
  in\n`data_50`.\n\nFirst, we need to create a vector of different `span` values.\n\n```
  r\nspans <- 2^seq(1, -2, -0.2)\n```\n\nNext, we can use purrr to iterate over the
  span values, fit a model for\neach one, and calculate the RMSE.\n\n``` r\nerrors
  <- \n  tibble(span = spans) %>% \n  mutate(\n    train_error =\n      map_dbl(span,
  ~ rmse(loess(y ~ x, data = data_1, span = .), data_1)),\n    test_error =\n      map_dbl(span,
  ~ rmse(loess(y ~ x, data = data_1, span = .), data_50))\n  )\n```\n\nThe result
  is a tibble with a training error and test error for each\n`span`.\n\n``` r\nerrors\n```\n\n
  \   ## # A tibble: 16 x 3\n    ##     span train_error test_error\n    ##    <dbl>
  \      <dbl>      <dbl>\n    ##  1 2            27.7       27.4\n    ##  2 1.74
  \        26.9       26.7\n    ##  3 1.52         25.9       25.7\n    ##  4 1.32
  \        24.6       24.6\n    ##  5 1.15         22.8       23.0\n    ##  6 1            21.3
  \      21.8\n    ##  7 0.871        19.3       20.4\n    ##  8 0.758        18.7
  \      20.2\n    ##  9 0.660        18.5       20.2\n    ## 10 0.574        18.4
  \      20.3\n    ## 11 0.5          18.4       20.3\n    ## 12 0.435        18.3
  \      20.4\n    ## 13 0.379        18.1       20.4\n    ## 14 0.330        17.8
  \      20.4\n    ## 15 0.287        17.4       20.6\n    ## 16 0.25         17.2
  \      20.8\n\nIt would be useful to compare these errors to the Bayes error, which
  you\ncan think of as the “best-case-scenario” error. The following code\ncalculates
  the Bayes error from `data_50`.\n\n``` r\nbayes_error <- \n  data_50 %>% \n  summarize(bayes_error
  = sqrt(mean((y - f(x))^2, na.rm = TRUE))) %>% \n  pull(bayes_error)\n\nbayes_error
  \n```\n\n    ## [1] 19.9741\n\nNow, let’s plot the training, test, and Bayes errors
  by `span`.\n\n``` r\nbayes <- tibble(\n  span = range(errors$span),\n  type = \"bayes_error\",\n
  \ error = bayes_error\n)\n\nerrors %>% \n  gather(key = type, value = error, -span)
  %>% \n  ggplot(aes(1 / span, error, color = type)) + \n  geom_line(data = bayes)
  +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Errors as a function
  of increasing model complexity\",\n    color = NULL\n  )\n```\n\n![](model-eval_files/figure-gfm/unnamed-chunk-13-1.png)<!--
  -->\n\nMapping 1 / `span` to the x-axis makes the plot easier to interpret\nbecause
  smaller values of `span` create more complex models. The\nleftmost points indicate
  the simplest models, and the rightmost indicate\nthe most complex.\n\nThe lowest
  test error is around 20.18, and came from the model with\n`span` ≈ 0.758. Note that
  the default value of `span` in `loess()` is\n0.75, so the default here would have
  done pretty well.\n\nYou can see that as the model increases in complexity, the
  training\nerror (blue line) continually declines, but the test error (green line)\nactually
  starts to increase. The point of divergence indicates the\n`span` values at which
  the model becomes too complex and starts\noverfitting the training data.\n\nWe used
  `data_50`, our new data, to calculate the actual test error. As\nwe’ve said, though,
  you usually won’t have access to new data and so\nwon’t be able to calculate the
  test error. You’ll need a way to\nestimate the test error using the training data.
  As our plot indicates,\nhowever, the training error is a poor estimate of the actual
  test error.\nIn the next section, we’ll discuss a better way, called\n*cross-validation*,
  to estimate the test error from the training data.\n\n## Cross-validation\n\nThere
  are two key ideas of cross-validation. First, in the absence of\nnew data, we can
  hold out of a random portion of our original data (the\ntest set), train the model
  on the rest of the data (the training set),\nand then test the model on test set.
  Second, we can generate multiple of\nthese train-test pairs to create a more robust
  estimate of the true test\nerror.\n\nmodelr provides two functions for generating
  these train-test pairs:\n`crossv_mc()` and `crossv_kfold()`. We’ll introduce these
  functions in\nthe next section.\n\n### `crossv_mc()` and `crossv_kfold()`\n\n`crossv_mc()`
  and `crossv_kfold()` both take your original data as input\nand produce a tibble
  that looks like this:\n\n``` r\ndata_1 %>% \n  crossv_kfold()\n```\n\n    ## # A
  tibble: 5 x 3\n    ##   train          test           .id  \n    ##   <list>         <list>
  \        <chr>\n    ## 1 <S3: resample> <S3: resample> 1    \n    ## 2 <S3: resample>
  <S3: resample> 2    \n    ## 3 <S3: resample> <S3: resample> 3    \n    ## 4 <S3:
  resample> <S3: resample> 4    \n    ## 5 <S3: resample> <S3: resample> 5\n\nEach
  row represents one train-test pair. Eventually, we’re going to\nbuild a model on
  each train portion, and then test that model on each\ntest portion.\n\n`crossv_mc()`
  and `crossv_kfold()` differ in how they generate the\ntrain-test pairs.\n\n`crossv_mc()`
  takes two parameters: `n` and `test`. `n` specifies how\nmany train-test pairs to
  generate. `test` specifies the portion of the\ndata to hold out each time. Importantly,
  `crossv_mc()` pulls randomly\nfrom the entire dataset to create each train-test
  split. This means that\nthe test sets are not mutually exclusive. A single point
  can be present\nin multiple test sets.\n\n`crossv_kfold()` takes one parameter:
  `k`, and splits the data into `k`\nexclusive partitions, or *folds*. It then uses
  each of these `k` folds\nas a test set, with the remaining `k` - 1 folds as the
  training set. The\nmain difference between `crossv_mc()` and `crossv_kfold()` is
  that the\n`crossv_kfold()` test sets are mutually exclusive.\n\nNow, let’s take
  a closer look at how these functions works. The\nfollowing code uses `crossv_kfold()`
  to create 10 train-test pairs.\n\n``` r\ndf <- \n  data_1 %>% \n  crossv_kfold(k
  = 10)\n\ndf\n```\n\n    ## # A tibble: 10 x 3\n    ##    train          test           .id
  \ \n    ##    <list>         <list>         <chr>\n    ##  1 <S3: resample> <S3:
  resample> 01   \n    ##  2 <S3: resample> <S3: resample> 02   \n    ##  3 <S3: resample>
  <S3: resample> 03   \n    ##  4 <S3: resample> <S3: resample> 04   \n    ##  5 <S3:
  resample> <S3: resample> 05   \n    ##  6 <S3: resample> <S3: resample> 06   \n
  \   ##  7 <S3: resample> <S3: resample> 07   \n    ##  8 <S3: resample> <S3: resample>
  08   \n    ##  9 <S3: resample> <S3: resample> 09   \n    ## 10 <S3: resample> <S3:
  resample> 10\n\nThe result is a tibble with two list-columns: `train` and `test`.
  Each\nelement of `train` and `test` is a resample object, which you might not\nhave
  seen before. Let’s pull out a single resample object and inspect\nit.\n\n``` r\nresample_1
  <- df$train[[1]]\nresample_1\n```\n\n    ## <resample [180 x 2]> 1, 3, 4, 5, 6,
  7, 8, 9, 10, 11, ...\n\nEach resample object is actually a two-element list.\n\n```
  r\nlength(resample_1)\n```\n\n    ## [1] 2\n\nThe first element, `data` is the original
  dataset.\n\n``` r\nresample_1$data\n```\n\n    ## # A tibble: 201 x 2\n    ##        x
  \     y\n    ##    <dbl>  <dbl>\n    ##  1   0    22.9 \n    ##  2   0.5  36.6 \n
  \   ##  3   1    -2.32\n    ##  4   1.5  21.3 \n    ##  5   2    18.9 \n    ##  6
  \  2.5  13.0 \n    ##  7   3   -18.3 \n    ##  8   3.5  18.5 \n    ##  9   4    29.0
  \n    ## 10   4.5   9.73\n    ## # … with 191 more rows\n\nThe second element, `idx`,
  is a set of indices that indicate the subset\nof the original data to use.\n\n```
  r\nindices <- resample_1$idx\nhead(indices)\n```\n\n    ## [1] 1 3 4 5 6 7\n\nWe
  can use a set operation to verify that the train and test sets are\nmutually exclusive.
  In other words, there is no point in a given train\nset that is also in its corresponding
  test set, and vice versa.\n\n``` r\nis_empty(intersect(df$train[[1]]$idx, df$test[[1]]$idx))\n```\n\n
  \   ## [1] TRUE\n\nThe union of each train and test set is just the complete original\ndataset.
  We can verify this by checking that the set of all train and\ntest indices contains
  every possible index in the original dataset.\n\n``` r\nsetequal(union(df$train[[1]]$idx,
  df$test[[1]]$idx),  1:201)\n```\n\n    ## [1] TRUE\n\nresample objects are not tibbles.
  Some model functions, like `lm()`, can\ntake resample objects as input, but `loess()`
  cannot. In order to use\n`loess()`, we’ll need to turn the resample objects into
  tibbles.\n\n``` r\nas_tibble(df$test[[1]])\n```\n\n    ## # A tibble: 21 x 2\n    ##
  \       x     y\n    ##    <dbl> <dbl>\n    ##  1   0.5  36.6\n    ##  2  15    82.6\n
  \   ##  3  20.5  82.3\n    ##  4  21    79.3\n    ##  5  27.5  98.3\n    ##  6  30
  \   80.2\n    ##  7  35.5 102. \n    ##  8  37    67.1\n    ##  9  39    79.0\n
  \   ## 10  41    41.3\n    ## # … with 11 more rows\n\nResample objects waste a
  lot of space, since each one contains the full\ndataset. A new package [rsample](https://topepo.github.io/rsample/)
  is\nbeing developed to support tidy modeling in a more space-efficient way.\n\n###
  Process\n\n`crossv_kfold()` and `crossv_mc()` just generate train-test pairs. In\norder
  to carry out cross-validation, you still need to fit your model on\neach train set
  and then evaluate it on each corresponding test set. In\nthis section, we’ll walk
  you through the process.\n\n**1** Create a function to calculate your test error\n\nFirst,
  you’ll need a function that calculates the test error using some\nmetric. We’ll
  use RMSE. The following function returns the RMSE for a\ngiven span, train set,
  and test set.\n\n``` r\nrmse_error <- function(span, train, test) {\n  rmse(loess(y
  ~ x, data = as_tibble(train), span = span), as_tibble(test))\n}\n```\n\n**2** Create
  a function to calculate CV error\n\nNext, you need a function that iterates over
  all your train-test pairs,\ncalculates the test error for each pair, and then calculates
  the mean\nand standard deviation of all the errors.\n\n``` r\nspan_error <- function(span,
  data_cv) {\n  errors <- \n    data_cv %>% \n    select(-.id) %>% \n    add_column(span
  = span) %>% \n    pmap_dbl(rmse_error)\n  \n  tibble(\n    span = span,\n    error_mean
  = mean(errors, rm = TRUE),\n    error_sd = sd(errors, na.rm = TRUE)\n  )\n}\n```\n\n**3**
  Create your train-test pairs\n\nUse either `crossv_mc()` or `crossv_kfold()` to
  generate your train-test\npairs. We’ll use `crossv_mc()` to generate 100 train-test
  pairs with\ntest sets consisting of approximately 20% of the data.\n\n``` r\nset.seed(430)\n\ndata_mc
  <- crossv_mc(data_1, n = 100, test = 0.2)\n```\n\n**4** Calculate the CV error for
  different values of your tuning\nparameter\n\nWe want to compare different values
  of `span`, so we also need to\niterate over a vector of different `span` values.
  We’ll calculate the CV\nerror for all `span`s using these train-test pairs.\n\n```
  r\nerrors_mc <- \n  spans %>% \n  map_dfr(~ span_error(span = ., data_cv = data_mc))\n\nerrors_mc
  %>% \n  knitr::kable()\n```\n\n|      span | error\\_mean | error\\_sd |\n| --------:
  | ----------: | --------: |\n| 2.0000000 |    28.13057 |  2.578456 |\n| 1.7411011
  |    27.32540 |  2.572414 |\n| 1.5157166 |    26.29811 |  2.566547 |\n| 1.3195079
  |    25.00708 |  2.558990 |\n| 1.1486984 |    23.06875 |  2.568471 |\n| 1.0000000
  |    21.62002 |  2.532316 |\n| 0.8705506 |    19.58527 |  2.417289 |\n| 0.7578583
  |    18.97282 |  2.323062 |\n| 0.6597540 |    18.84846 |  2.269925 |\n| 0.5743492
  |    18.86447 |  2.256472 |\n| 0.5000000 |    18.90930 |  2.260496 |\n| 0.4352753
  |    18.93128 |  2.254325 |\n| 0.3789291 |    18.88556 |  2.220315 |\n| 0.3298770
  |    18.76699 |  2.171678 |\n| 0.2871746 |    18.60700 |  2.135667 |\n| 0.2500000
  |    18.52702 |  2.109205 |\n\n**5** Choose the best model\n\nNow, we need to compare
  the different models and choose the best value\nof `span`.\n\nBecause we simulated
  our data, we can compare the CV estimates of the\ntest error with the actual test
  error.\n\n``` r\nerrors_mc %>% \n  left_join(errors, by = \"span\") %>% \n  ggplot(aes(1
  / span)) +\n  geom_line(aes(y = test_error, color = \"test_error\")) +\n  geom_point(aes(y
  = test_error, color = \"test_error\")) +\n  geom_line(aes(y = error_mean, color
  = \"mc_error\")) +\n  geom_pointrange(\n    aes(\n      y = error_mean, \n      ymin
  = error_mean - error_sd,\n      ymax = error_mean + error_sd,\n      color = \"mc_error\"\n
  \   )\n  ) +\n  labs(\n    title = \"Cross-validation and test errors\",\n    y
  = \"error\",\n    color = NULL\n  )\n```\n\n![](model-eval_files/figure-gfm/unnamed-chunk-27-1.png)<!--
  -->\n\nFrom this plot, we can see that CV error estimates from `crossv_mc()`\nunderestimate
  the true test error in the range with 1 / `span` \\>= 1.\nThe line ranges reflect
  one standard error on either side of the mean\nerror and show considerable uncertainty
  in the error estimates. Except\nfor the largest 1 / `span` = 4, the test error is
  within one standard\nerror of the mean.\n\nUsually, you won’t have access to the
  true test error, so we need a way\nto choose the best value of `span` using just
  the CV errors. Here’s the\nrule-of-thumb for choosing a tuning parameter when you
  only know the CV\nerrors:\n\n  - Start with the parameter that has the lowest mean
  CV error. In this\n    case, that would be `span` = 0.25 (on the plot, `1/span`
  = 4).\n  - Now, imagine sliding the one-standard-error range for `span` = 0.25\n
  \   all the way to the left of the plot. Now, start sliding the range to\n    the
  right until the first CV mean error is within its bounds. In our\n    case, the
  top of the one-standard-error range for `span` = 0.25 is\n    approximately 20.6.
  The CV mean error for `span` = 1 is larger than\n    this, but the next most complex
  model with `span` ≈ 0.871 is\n    smaller, so we would choose this `span`.\n\nWe
  saw above that if we knew the test error, we would choose `span` ≈\n0.758 as the
  optimal parameter. But we usually don’t know the test\nerror, and in this case the
  test errors for 0.871 and 0.758 were very\nclose, with both quite close to the Bayes
  error of 20.\n\n``` r\nerrors %>% \n  filter(span < 0.9, span > 0.7) %>% \n  select(span,
  test_error) %>% \n  knitr::kable()\n```\n\n|      span | test\\_error |\n| --------:
  | ----------: |\n| 0.8705506 |     20.4014 |\n| 0.7578583 |     20.1787 |\n"
