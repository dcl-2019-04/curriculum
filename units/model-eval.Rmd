---
unit_title: Model evaluation
theme: model
needs: [model-multivariate]
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, message = FALSE}
library(tidyverse)
library(modelr)
```

A crucial part of the modeling process is determining the quality of your model. Often, you'll want to compare different models to each other to figure out which one fits your data the best. In this reading, we'll explore how to compare different models to each other and pick the highest quality one.

## The problem

In order to illustrate the process of model evaluation, we're going to use simulated data. To simulate data, you pick a function and use that function to generate a series of data points. The advantage of using simulated data is therefore that we actually know the functional representation of the data, since we were the ones to create the data. We can then compare any models we make to the true function. 

Note that you almost always won't actually know the functional representation of your data. If you did, you probably wouldn't need a model in the first place. 

For our simulated data, let's assume there's some phenomenon that has a functional representation of the form

$$y = f(x)$$

In other words, the phenomenon is purely a function of the variable `x`, and so, if you know `x`, you can figure out the behavior of the phenomenon. Here's the $f$ that we'll use in this reading.

```{r}
# True function
f <- function(x) x + 50 * sin((pi / 50) * x)
```

$f$ is the true function, but let's also assume that any measurements of $y$ will have errors. To simulate errors, we need a function that adds random noise to $f$. We'll call this function $g$. 

```{r}
set.seed(439)

# Function with measurement error
g <- function(x) f(x) + rnorm(n = length(x), mean = 0, sd = 20)
```

We can use $g$ to generate a random sample of data.

```{r}
# Function that creates a random sample of data points, using g(x)
sim_data <- function(from, to, by) {
  tibble(
    x = seq(from = from, to = to, by = by),
    y = g(x)
  )
}

data_1 <- sim_data(0, 100, 0.5)
```

Here's a plot of both the simulated data (`data_1`), and the true function. Our job is now to use the simulated data to create a function that hopefully comes close to matching the true function.

```{r}
tibble(x = seq(0, 100, 0.5), y = f(x)) %>% 
  ggplot(aes(x, y)) +
  geom_line() +
  geom_point(data = data_1) +
  labs(title = "True function and simulated data")
```

## The `loess()` model

Given the shape of our data, the `stats::loess()` function is a good place to start. `loess()` works by by creating a local model at each data point. Each of these local models only uses data within a given distance from that point. The `span` parameter controls how many points are included in each of these local models. The smaller the span, the fewer the number of points included in each local model. 

To better understand the effect of the `span` parameter, let's plot `loess()` models with different values of `span`. 

The function `plot_loess()` fits a model using `loess()`, then plots the model predictions and the underlying data.

```{r}
# Plot loess model for given span
plot_loess <- function(span) {
  data_1 %>% 
    mutate(f = f(x)) %>% 
    add_predictions(loess(y ~ x, data = data_1, span = span)) %>% 
    ggplot(aes(x)) +
    geom_line(aes(y = f)) +
    geom_point(aes(y = y)) +
    geom_line(aes(y = pred), color = "#3366FF", size = 1) +
    labs(
      title = str_c("loess, span = ", span),
      y = "y"
    )
}
```

Now, we can use purrr to apply `plot_loess()` to a vector of different span values.

```{r, warning=FALSE}
c(10, 1, 0.75, 0.1, 0.01) %>% 
  map(plot_loess) %>% 
  walk(print)
```

The `span` parameter clearly makes a big difference in the model:

* `span = 10` smooths too much and washes out the variability in the model.
* `span = 1` is much better, but still has a bit too much smoothing.
* `span = 0.75` is the default and does a reasonably good job.
* `span = 0.1` has too little smoothing. The added complexity has started tracking the random variation in our data.
* `span = 0.01` went way too far and started to fit individual random points.

It's clear that the model with `span = 0.75` comes closest to following the form of the true model. The models with `span` equal to `0.1` and `0.01` have __overfit__ the data, and so won't generalize well to new data. 

## Test and training errors

Let's return to the question of how to measure the quality of a model. Above, we could easily visualize our different models because they each had only one predictor variable, `x`. It will be much harder to visualize models with many predictor variables, and so we need a more general way of determining model quality and comparing models to each other.

One way to measure model quality involves comparing the model's predicted values to the actual values in data. Ideally, we want to know the difference between predicted and actual values on _new_ data. This difference is called the __test error__. 

Unfortunately, you usually won't have new data. All you'll have is the data used to create the model. The difference between the predicted and actual values on the data used to train the model is called the __training error__.

Since the expected value for the error term of `g()` is `0` for each `x`, the best possible model for new data would be `f()` itself. The error on `f()` itself is called the __Bayes error__. 

Usually, you won't know the true function `f()`, and you won't have access to new test data. In our case, we simulated data, so we know `f()` and can generate new data. The following code creates 50 different samples, each the same size. We'll use these 50 different samples to get an accurate estimate of our test error.

```{r}
set.seed(886)

data_50 <- 
  1:50 %>% 
  map_dfr(~ sim_data(0, 100, 0.5))
```

Now, we need a way to measure the difference between a vector of predictions and vector of actual values. Two common metrics include:

* Root-mean-square error (RMSE): `sqrt(mean((y - pred)^2, na.rm = TRUE))`
* Mean absolute error (MAE): `mean(abs(y - pred), na.rm = TRUE)`

Both measures are supported by modelr. We'll use RMSE below.

Now, we'll train a series of models on our original dataset, `data_1`, using a range of `span`'s. Then, for each model, we'll calculate the training error on `data_1` and the test error on the new data in `data_50`.

First, we need to create a vector of different `span` values. 

```{r}
spans <- 2^seq(1, -2, -0.2)
```

Next, we can use purrr to iterate over the span values, fit a model for each one, and calculate the RMSE.

```{r}
errors <- 
  tibble(span = spans) %>% 
  mutate(
    train_error =
      map_dbl(span, ~ rmse(loess(y ~ x, data = data_1, span = .), data_1)),
    test_error =
      map_dbl(span, ~ rmse(loess(y ~ x, data = data_1, span = .), data_50))
  )
```

The result is a tibble with a training error and test error for each `span`.

```{r}
errors
```

It would be useful to compare these errors to the Bayes error, which you can think of as the "best-case-scenario" error. The following code calculates the Bayes error from `data_50`.

```{r}
bayes_error <- 
  data_50 %>% 
  summarize(bayes_error = sqrt(mean((y - f(x))^2, na.rm = TRUE))) %>% 
  pull(bayes_error)

bayes_error 
```

Now, let's plot the training, test, and Bayes errors by `span`. 

```{r}
bayes <- tibble(
  span = range(errors$span),
  type = "bayes_error",
  error = bayes_error
)

errors %>% 
  gather(key = type, value = error, -span) %>% 
  ggplot(aes(1 / span, error, color = type)) + 
  geom_line(data = bayes) +
  geom_line() +
  geom_point() +
  labs(
    title = "Errors as a function of increasing model complexity",
    color = NULL
  )
```

Mapping 1 / `span` to the x-axis makes the plot easier to interpret because smaller values of `span` create more complex models. The leftmost points indicate the simplest models, and the rightmost indicate the most complex. 

The lowest test error is around 20.18, and came from the model with `span` ≈ 0.758. Note that the default value of `span` in `loess()` is 0.75, so the default here would have done pretty well. 

You can see that as the model increases in complexity, the training error (blue line) continually declines, but the test error (green line) actually starts to increase. The point of divergence indicates the `span` values at which the model becomes too complex and starts overfitting the training data.

We used `data_50`, our new data, to calculate the actual test error. As we've said, though, you usually won't have access to new data and so won't be able to calculate the test error. You'll need a way to estimate the test error using the training data. As our plot indicates, however, the training error is a poor estimate of the actual test error. In the next section, we'll discuss a better way, called _cross-validation_, to estimate the test error from the training data. 

## Cross-validation

There are two key ideas of cross-validation. First, in the absence of new data, we can hold out of a random portion of our original data (the test set), train the model on the rest of the data (the training set), and then test the model on test set. Second, we can generate multiple of these train-test pairs to create a more robust estimate of the true test error. 

modelr provides two functions for generating these train-test pairs: `crossv_mc()` and `crossv_kfold()`. We'll introduce these functions in the next section. 

### `crossv_mc()` and `crossv_kfold()`

`crossv_mc()` and `crossv_kfold()` both take your original data as input and produce a tibble that looks like this:

```{r}
data_1 %>% 
  crossv_kfold()
```

Each row represents one train-test pair. Eventually, we're going to build a model on each train portion, and then test that model on each test portion. 

`crossv_mc()` and `crossv_kfold()` differ in how they generate the train-test pairs.

`crossv_mc()` takes two parameters: `n` and `test`. `n` specifies how many train-test pairs to generate. `test` specifies the portion of the data to hold out each time. Importantly, `crossv_mc()` pulls randomly from the entire dataset to create each train-test split. This means that the test sets are not mutually exclusive. A single point can be present in multiple test sets.

`crossv_kfold()` takes one parameter: `k`, and splits the data into `k` exclusive partitions, or _folds_. It then uses each of these `k` folds as a test set, with the remaining `k` - 1 folds as the training set. The main difference between `crossv_mc()` and `crossv_kfold()` is that the `crossv_kfold()` test sets are mutually exclusive.

Now, let's take a closer look at how these functions works. The following code uses `crossv_kfold()` to create 10 train-test pairs.  

```{r}
df <- 
  data_1 %>% 
  crossv_kfold(k = 10)

df
```

The result is a tibble with two list-columns:  `train` and `test`. Each element of `train` and `test` is a resample object, which you might not have seen before. Let's pull out a single resample object and inspect it.

```{r}
resample_1 <- df$train[[1]]
resample_1
```

Each resample object is actually a two-element list. 

```{r}
length(resample_1)
```

The first element, `data` is the original dataset.

```{r}
resample_1$data
```

The second element, `idx`, is a set of indices that indicate the subset of the original data to use.

```{r}
indices <- resample_1$idx
head(indices)
```

We can use a set operation to verify that the train and test sets are mutually exclusive. In other words, there is no point in a given train set that is also in its corresponding test set, and vice versa.

```{r}
is_empty(intersect(df$train[[1]]$idx, df$test[[1]]$idx))
```

The union of each train and test set is just the complete original dataset. We can verify this by checking that the set of all train and test indices contains every possible index in the original dataset.

```{r}
setequal(union(df$train[[1]]$idx, df$test[[1]]$idx),  1:201)
```

resample objects are not tibbles. Some model functions, like `lm()`, can take resample objects as input, but `loess()` cannot. In order to use `loess()`, we'll need to turn the resample objects into tibbles. 

```{r}
as_tibble(df$test[[1]])
```

Resample objects waste a lot of space, since each one contains the full dataset. A new package [rsample](https://topepo.github.io/rsample/) is being developed to support tidy modeling in a more space-efficient way.

### Process

`crossv_kfold()` and `crossv_mc()` just generate train-test pairs. In order to carry out cross-validation, you still need to fit your model on each train set and then evaluate it on each corresponding test set. In this section, we'll walk you through the process.

__1__ Create a function to calculate your test error

First, you'll need a function that calculates the test error using some metric. We'll use RMSE. The following function returns the RMSE for a given span, train set, and test set.

```{r}
rmse_error <- function(span, train, test) {
  rmse(loess(y ~ x, data = as_tibble(train), span = span), as_tibble(test))
}
```

__2__ Create a function to calculate CV error

Next, you need a function that iterates over all your train-test pairs, calculates the test error for each pair, and then calculates the mean and standard deviation of all the errors. 

```{r}
span_error <- function(span, data_cv) {
  errors <- 
    data_cv %>% 
    select(-.id) %>% 
    add_column(span = span) %>% 
    pmap_dbl(rmse_error)
  
  tibble(
    span = span,
    error_mean = mean(errors, rm = TRUE),
    error_sd = sd(errors, na.rm = TRUE)
  )
}
```

__3__ Create your train-test pairs

Use either `crossv_mc()` or `crossv_kfold()` to generate your train-test pairs. We'll use `crossv_mc()` to generate 100 train-test pairs with test sets consisting of approximately 20% of the data.

```{r}
set.seed(430)

data_mc <- crossv_mc(data_1, n = 100, test = 0.2)
```

__4__ Calculate the CV error for different values of your tuning parameter

We want to compare different values of `span`, so we also need to iterate over a vector of different `span` values. We'll calculate the CV error for all `span`s using these train-test pairs.

```{r}
errors_mc <- 
  spans %>% 
  map_dfr(~ span_error(span = ., data_cv = data_mc))

errors_mc %>% 
  knitr::kable()
```

__5__ Choose the best model

Now, we need to compare the different models and choose the best value of `span`.

Because we simulated our data, we can compare the CV estimates of the test error with the actual test error.

```{r}
errors_mc %>% 
  left_join(errors, by = "span") %>% 
  ggplot(aes(1 / span)) +
  geom_line(aes(y = test_error, color = "test_error")) +
  geom_point(aes(y = test_error, color = "test_error")) +
  geom_line(aes(y = error_mean, color = "mc_error")) +
  geom_pointrange(
    aes(
      y = error_mean, 
      ymin = error_mean - error_sd,
      ymax = error_mean + error_sd,
      color = "mc_error"
    )
  ) +
  labs(
    title = "Cross-validation and test errors",
    y = "error",
    color = NULL
  )
```

From this plot, we can see that CV error estimates from `crossv_mc()` underestimate the true test error in the range with 1 / `span` >= 1. The line ranges reflect one standard error on either side of the mean error and show considerable uncertainty in the error estimates. Except for the largest 1 / `span` = 4, the test error is within one standard error of the mean.

Usually, you won't have access to the true test error, so we need a way to choose the best value of `span` using just the CV errors. Here's the rule-of-thumb for choosing a tuning parameter when you only know the CV errors:

* Start with the parameter that has the lowest mean CV error. In this case, that would be `span` = 0.25 (on the plot, `1/span` = 4).
* Now, imagine sliding the one-standard-error range for `span` = 0.25 all the way to the left of the plot. Now, start sliding the range to the right until the first CV mean error is within its bounds. In our case, the top of the one-standard-error range for `span` = 0.25 is approximately 20.6. The CV mean error for `span` = 1 is larger than this, but the next most complex model with `span` ≈ 0.871 is smaller, so we would choose this `span`.

We saw above that if we knew the test error, we would choose `span` ≈ 0.758 as the optimal parameter. But we usually don't know the test error, and in this case the test errors for 0.871 and 0.758 were very close, with both quite close to the Bayes error of 20.

```{r}
errors %>% 
  filter(span < 0.9, span > 0.7) %>% 
  select(span, test_error) %>% 
  knitr::kable()
```
